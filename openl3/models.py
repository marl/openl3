import os
import warnings
from openl3.openl3_exceptions import OpenL3Error

with warnings.catch_warnings():
    # Suppress TF and Keras warnings when importing
    warnings.simplefilter("ignore")
    from kapre.time_frequency import Spectrogram, Melspectrogram
    from keras.layers import (
        Input, Conv2D, BatchNormalization, MaxPooling2D,
        Flatten, Activation, Lambda
    )
    from keras.models import Model
    import keras.regularizers as regularizers
    from keras.utils.conv_utils import conv_output_length

_get_factors = lambda x: (
    (i, int(x/i)) for i in range(int(x**0.5), 0, -1) if not x % i)


class OpenL3Linear:
    # INPUTS
    precomputed = False
    n_dft = 512
    n_hop = 242
    sr = 48000
    audio_window_dur = 1

    weight_decay = 1e-5

    # CONV BLOCK 1
    n_filter_a_1 = 64
    filt_size_a_1 = (3, 3)
    pool_size_a_1 = (2, 2)

    # CONV BLOCK 2
    n_filter_a_2 = 128
    filt_size_a_2 = (3, 3)
    pool_size_a_2 = (2, 2)

    # CONV BLOCK 3
    n_filter_a_3 = 256
    filt_size_a_3 = (3, 3)
    pool_size_a_3 = (2, 2)

    # CONV BLOCK 4
    n_filter_a_4 = 512
    filt_size_a_4 = (3, 3)

    # pool_size_emb = {
    #     6144: (8, 8),
    #     512: (32, 24),
    # }

    embedding_size = 512
    input_repr = 'linear'
    content_type = 'env'

    def __init__(self, **kw):
        # TODO: formalize argument handling so that the docs can be autogenerated.
        for k in kw:
            # only replace non-method variables that already exist ()
            if hasattr(self, k) and not callable(getattr(self, k)):
                setattr(self, k, kw[k])
            else:
                OpenL3Error('Invalid model parameter:', k)

        self.model = None

        if str(self.input_repr) not in ("linear", "mel128", "mel256"):
            raise OpenL3Error('Invalid input representation "{}"'.format(self.input_repr))

        if str(self.content_type) not in ("music", "env"):
            raise OpenL3Error('Invalid content type "{}"'.format(self.content_type))

        if self.embedding_size not in (6144, 512):
            raise OpenL3Error('Invalid content type "{}"'.format(self.embedding_size))

    @property
    def weights_file(self):
        '''The path to the weights file for a certain input representation and (input_repr) and '''
        return os.path.join(
            os.path.dirname(__file__),
            'openl3_audio_{}_{}.h5'.format(self.input_repr, self.content_type))

    @property
    def _n_channels(self):
        '''number of frequency channels in the spectrogram input'''
        return self.n_dft // 2 + 1

    @property
    def _n_spec_frames(self):
        '''number of frames in the spectrogram input'''
        # taken from: https://github.com/keunwoochoi/kapre/blob/8517f45d3ccb9fff1ec0049b9c3e4389f81c20aa/kapre/time_frequency.py#L112
        return conv_output_length(self.sr * self.audio_window_dur,
                                  self.n_dft,
                                  'same', # padding
                                  # NOTE: Padding SHOULD be set to 'valid' (as
                                  #       is passed to kapre.Spectrogram), but
                                  #       that was giving invalid spectrogram
                                  #       sizes. Hmm..
                                  self.n_hop)

    @property
    def spec_input_shape(self):
        '''The spectrogram'''
        return self._n_channels, self._n_spec_frames, 1

    @property
    def pool_size_embedding(self):
        '''Calculate the pooling size for the final layer to get the desired embedding shape.'''
        shp_f, shp_t = self.spec_input_shape[:2]

        # calculate the size after pooling (assuming padding is 'same')
        pooling_layers = [self.pool_size_a_1, self.pool_size_a_2, self.pool_size_a_3]
        for pool_f, pool_t in pooling_layers:
            shp_f, shp_t = shp_f // pool_f, shp_t // pool_t

        # calculate the amount to pool along each axis
        scaling_factor = self.embedding_size / self.n_filter_a_4
        try:
            # we need integer factors to scale, otherwise it won't work.
            factors = _get_factors(scaling_factor)
            if shp_f > shp_t: # factors are returned from lowest to highest, so swap order.
                factors = ((b, a) for a, b in factors)

            # scaling factors
            s_f, s_t = next((scale_f, scale_t) for scale_f, scale_t in factors
                            if not shp_f % scale_f and not shp_t % scale_t)

        except StopIteration:
            raise OpenL3Error('Embedding of size {} could not be created from '
                              'a output channel size of {}'.format(
                              self.embedding_size, self.n_filter_a_4))

        return int(shp_f / s_f), int(shp_t / s_t)


    def build(self):
        '''Build the model and load pretrained weights.

        Returns
        -------
        self
        '''
        m = self.build_network()

        m.load_weights(self.weights_file)
        y_a = MaxPooling2D(pool_size=self.pool_size_embedding, padding='same')(m.output)
        y_a = Flatten()(y_a)

        self.model = Model(inputs=m.input, outputs=y_a)
        return self

    def build_network(self):
        """
        Returns an uninitialized model object for a network with a Mel
        spectrogram input (with 128 frequency bins). No final pooling/flattening
        is performed.

        Returns
        -------
        model : keras.models.Model
            Model object.
        """

        # create input
        if self.precomputed: # use precomputed spectrograms
            x_a = y_a = Input(shape=self.spec_input_shape, dtype='float32')
        else:
            x_a = Input(shape=(1, self.sr * self.audio_window_dur), dtype='float32')
            y_a = self._build_spectrogram_features(x_a)

        y_a = BatchNormalization()(y_a)

        # CONV BLOCK 1
        y_a = self._build_audio_conv_layer(
            y_a, self.n_filter_a_1, self.filt_size_a_1)

        y_a = self._build_audio_conv_layer(
            y_a, self.n_filter_a_1, self.filt_size_a_1, self.pool_size_a_1)

        # CONV BLOCK 2
        y_a = self._build_audio_conv_layer(
            y_a, self.n_filter_a_2, self.filt_size_a_2)

        y_a = self._build_audio_conv_layer(
            y_a, self.n_filter_a_2, self.filt_size_a_2, self.pool_size_a_2)

        # CONV BLOCK 3
        y_a = self._build_audio_conv_layer(
            y_a, self.n_filter_a_3, self.filt_size_a_3)

        y_a = self._build_audio_conv_layer(
            y_a, self.n_filter_a_3, self.filt_size_a_3, self.pool_size_a_3)

        # CONV BLOCK 4
        y_a = self._build_audio_conv_layer(
            y_a, self.n_filter_a_4, self.filt_size_a_4)

        y_a = Conv2D(self.n_filter_a_4, self.filt_size_a_4, padding='same',
                     kernel_initializer='he_normal', name='audio_embedding_layer',
                     kernel_regularizer=regularizers.l2(self.weight_decay))(y_a)

        # y_a = MaxPooling2D(pool_size=self.pool_size_embedding, padding='same')(y_a)
        # y_a = Flatten()(y_a)
        m = Model(inputs=x_a, outputs=y_a)
        return m

    def _build_spectrogram_features(self, x_a):
        '''Take audio frames as input and convert them to linear spectrogram windows'''
        # SPECTROGRAM PREPROCESSING
        # 257 x 199 x 1 (for defaults)
        return Spectrogram(n_dft=self.n_dft, n_hop=self.n_hop, power_spectrogram=1.0,
                           return_decibel_spectrogram=True, padding='valid')(x_a)

    def _build_audio_conv_layer(self, y_a, n_filter, filt_size, pool_size=None):
        '''Builds a single convolutional block of the L3 network'''
        # CONV BLOCK
        y_a = Conv2D(n_filter, filt_size, padding='same',
                     kernel_initializer='he_normal',
                     kernel_regularizer=regularizers.l2(self.weight_decay))(y_a)
        y_a = BatchNormalization()(y_a)
        y_a = Activation('relu')(y_a)

        if pool_size: # optional pooling layer
            y_a = MaxPooling2D(pool_size=pool_size, strides=2)(y_a)

        return y_a


class OpenL3Mel128(OpenL3Linear):
    n_dft = 2048
    n_mels = 128
    embedding_size = 512
    input_repr = 'mel128'

    @property
    def _n_channels(self):
        '''number of frequency channels in the spectrogram input'''
        return self.n_mels

    def _build_spectrogram_features(self, x_a):
        '''Take audio frames as input and convert them to mel spectrogram windows'''
        # MELSPECTROGRAM PREPROCESSING
        # 128 x 199 x 1
        return Melspectrogram(n_dft=self.n_dft, n_hop=self.n_hop, n_mels=self.n_mels,
                              sr=self.sr, power_melgram=1.0, htk=True,
                              return_decibel_melgram=True, padding='same')(x_a)


class OpenL3Mel256(OpenL3Mel128):
    n_dft = 2048
    n_mels = 256
    embedding_size = 512
    input_repr = 'mel256'





'''



Previous API


'''


POOLINGS = {
    'linear': {
        6144: (8, 8),
        512: (32, 24),
    },
    'mel128': {
        6144: (4, 8),
        512: (16, 24),
    },
    'mel256': {
        6144: (8, 8),
        512: (32, 24),
    }
}


def load_embedding_model(input_repr, content_type, embedding_size):
    """
    Returns a model with the given characteristics. Loads the model
    if the model has not been loaded yet.

    Parameters
    ----------
    input_repr : "linear", "mel128", or "mel256"
        Spectrogram representation used for model.
    content_type : "music" or "env"
        Type of content used to train embedding.
    embedding_size : 6144 or 512
        Embedding dimensionality.

    Returns
    -------
    model : keras.models.Model
        Model object.
    """

    # Construct embedding model and load model weights
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        Model = MODEL_CLASSES[input_repr]
        m = Model(content_type=content_type, embedding_size=embedding_size).build()
        return m.model


def load_embedding_model_path(input_repr, content_type):
    """
    Returns the local path to the model weights file for the model
    with the given characteristics

    Parameters
    ----------
    input_repr : "linear", "mel128", or "mel256"
        Spectrogram representation used for model.
    content_type : "music" or "env"
        Type of content used to train embedding.

    Returns
    -------
    output_path : str
        Path to given model object
    """
    Model = MODEL_CLASSES[input_repr]
    return Model(content_type=content_type).weights_file


def _construct_linear_audio_network():
    """
    Returns an uninitialized model object for a network with a linear
    spectrogram input (With 257 frequency bins)

    Returns
    -------
    model : keras.models.Model
        Model object.
    """

    return OpenL3Linear().build_network()


def _construct_mel128_audio_network():
    """
    Returns an uninitialized model object for a network with a Mel
    spectrogram input (with 128 frequency bins).

    Returns
    -------
    model : keras.models.Model
        Model object.
    """


    return OpenL3Mel128().build_network()


def _construct_mel256_audio_network():
    """
    Returns an uninitialized model object for a network with a Mel
    spectrogram input (with 256 frequency bins).

    Returns
    -------
    model : keras.models.Model
        Model object.
    """


    return OpenL3Mel256().build_network()

MODEL_CLASSES = {
    'linear': OpenL3Linear,
    'mel128': OpenL3Mel128,
    'mel256': OpenL3Mel256
}

MODELS = {
    'linear': _construct_linear_audio_network,
    'mel128': _construct_mel128_audio_network,
    'mel256': _construct_mel256_audio_network
}
